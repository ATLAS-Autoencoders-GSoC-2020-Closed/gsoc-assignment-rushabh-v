{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPR5JkX3to1uwoUAl9L0sRz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rushabh-v/AE-4-to-3/blob/master/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgm8owaT_tBh",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uZ3RziH7aIq",
        "colab_type": "code",
        "outputId": "4aa406fd-0667-45eb-913c-f8a3d7bdcc3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkT-jaSoTiOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from fastai import basic_train, basic_data\n",
        "from fastai.basic_data import DatasetType\n",
        "import fastai\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRxiZlGq_2si",
        "colab_type": "text"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYhUACd6VoIt",
        "colab_type": "code",
        "outputId": "65b61384-923d-482e-a0b7-2da9fd615ca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train = pd.read_pickle('/content/all_jets_train_4D_100_percent.pkl')\n",
        "test = pd.read_pickle('/content/all_jets_test_4D_100_percent.pkl')\n",
        "\n",
        "train.head(5)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>m</th>\n",
              "      <th>pt</th>\n",
              "      <th>phi</th>\n",
              "      <th>eta</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>132784</th>\n",
              "      <td>3831.839355</td>\n",
              "      <td>22000.609375</td>\n",
              "      <td>1.567018</td>\n",
              "      <td>1.142924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99666</th>\n",
              "      <td>4582.417480</td>\n",
              "      <td>21648.210938</td>\n",
              "      <td>-2.680558</td>\n",
              "      <td>0.213654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26629</th>\n",
              "      <td>16747.765625</td>\n",
              "      <td>169514.281250</td>\n",
              "      <td>-1.948239</td>\n",
              "      <td>1.163296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80473</th>\n",
              "      <td>14789.586914</td>\n",
              "      <td>183085.609375</td>\n",
              "      <td>-1.641102</td>\n",
              "      <td>2.670927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48229</th>\n",
              "      <td>4646.724121</td>\n",
              "      <td>20527.130859</td>\n",
              "      <td>2.922270</td>\n",
              "      <td>-1.158871</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   m             pt       phi       eta\n",
              "132784   3831.839355   22000.609375  1.567018  1.142924\n",
              "99666    4582.417480   21648.210938 -2.680558  0.213654\n",
              "26629   16747.765625  169514.281250 -1.948239  1.163296\n",
              "80473   14789.586914  183085.609375 -1.641102  2.670927\n",
              "48229    4646.724121   20527.130859  2.922270 -1.158871"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YyoeFMyAKHV",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "### Normalization\n",
        "Make the inputs having `mean` of `0` and `standard deviation`\n",
        "of `1`. \n",
        "\n",
        "This fixes the outliers in the data and makes the learning process much more easier, smoother and faster for the computer.\n",
        "\n",
        "note: All the data(e.g `train` and `test` data) should be normalized with the same mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhxVlo6ZT21P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train.mean()\n",
        "std = train.std()\n",
        "\n",
        "train_data = (train - mean) / std\n",
        "test_data = (test - mean) / std\n",
        "\n",
        "train_x = train_data\n",
        "test_x = test_data\n",
        "train_y = train_x\n",
        "test_y = test_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX5Ln77FB1K2",
        "colab_type": "text"
      },
      "source": [
        "### SVD [(Singular Value Decomposition)](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
        "\n",
        "SVD is a mathematical process(basically metrix operations) to factorize a matrix into a lower dimantion of our choise.\n",
        "\n",
        "Here, I have specified `n_components=3` below as the lower dimention, as we want to compress our `4` variable tensor into `3` variable tensor.\n",
        "\n",
        "More details and tutorials on `svd` can be found on Rachel Thomas's(__Fastai's__) __Computational Linear Algebra Course__. [Notebook](https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb#Singular-Value-Decomposition-(SVD)) and the [lecture](https://www.youtube.com/watch?v=kgd40iDT8yY&list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY&index=2) can be found in the respective links.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcyQ94KlMD4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svd = TruncatedSVD(n_components=3)\n",
        "\n",
        "def add_singular_values(df1):\n",
        "\n",
        "    df = df1.copy() # make a copy of the data\n",
        "    sing_vals = []\n",
        "\n",
        "    # compute and append the singular values into list sing_vals for each data entry\n",
        "    for i in range(len(df)):\n",
        "        a = np.diag(df.iloc[i])\n",
        "        svd.fit(a)\n",
        "        sing_vals.append(svd.singular_values_)\n",
        "\n",
        "    # add the singular values into DataFrame\n",
        "    for i in range(3):\n",
        "        df.insert(len(df.columns), f'sv_{i}', np.array(sing_vals)[:,i])\n",
        "\n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxCYBhFmQvtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = add_singular_values(train_x)\n",
        "test_df = add_singular_values(test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co0q7A2XWaOd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "afbdf5ae-e004-4277-e8e5-e2aad7522ca8"
      },
      "source": [
        "train_df.head(5) # sv_0, sv_1 and sv_2 columns in the below DataFrame are respective singular values."
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>m</th>\n",
              "      <th>pt</th>\n",
              "      <th>phi</th>\n",
              "      <th>eta</th>\n",
              "      <th>sv_0</th>\n",
              "      <th>sv_1</th>\n",
              "      <th>sv_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>132784</th>\n",
              "      <td>-0.688496</td>\n",
              "      <td>-0.607629</td>\n",
              "      <td>0.868107</td>\n",
              "      <td>0.759040</td>\n",
              "      <td>0.868107</td>\n",
              "      <td>0.759040</td>\n",
              "      <td>0.688496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99666</th>\n",
              "      <td>-0.587358</td>\n",
              "      <td>-0.612672</td>\n",
              "      <td>-1.487534</td>\n",
              "      <td>0.117474</td>\n",
              "      <td>1.487534</td>\n",
              "      <td>0.612672</td>\n",
              "      <td>0.587358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26629</th>\n",
              "      <td>1.051897</td>\n",
              "      <td>1.503479</td>\n",
              "      <td>-1.081401</td>\n",
              "      <td>0.773105</td>\n",
              "      <td>1.503479</td>\n",
              "      <td>1.081401</td>\n",
              "      <td>1.051897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80473</th>\n",
              "      <td>0.788036</td>\n",
              "      <td>1.697702</td>\n",
              "      <td>-0.911068</td>\n",
              "      <td>1.813972</td>\n",
              "      <td>1.813972</td>\n",
              "      <td>1.697702</td>\n",
              "      <td>0.911068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48229</th>\n",
              "      <td>-0.578692</td>\n",
              "      <td>-0.628716</td>\n",
              "      <td>1.619709</td>\n",
              "      <td>-0.830115</td>\n",
              "      <td>1.619709</td>\n",
              "      <td>0.830115</td>\n",
              "      <td>0.628716</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               m        pt       phi       eta      sv_0      sv_1      sv_2\n",
              "132784 -0.688496 -0.607629  0.868107  0.759040  0.868107  0.759040  0.688496\n",
              "99666  -0.587358 -0.612672 -1.487534  0.117474  1.487534  0.612672  0.587358\n",
              "26629   1.051897  1.503479 -1.081401  0.773105  1.503479  1.081401  1.051897\n",
              "80473   0.788036  1.697702 -0.911068  1.813972  1.813972  1.697702  0.911068\n",
              "48229  -0.578692 -0.628716  1.619709 -0.830115  1.619709  0.830115  0.628716"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiiigFcMX6Lx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ca3ded15-3fbd-4c1f-9b3d-429f2133546a"
      },
      "source": [
        "train_y.head(5) # here are the respective targets for the above data"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>m</th>\n",
              "      <th>pt</th>\n",
              "      <th>phi</th>\n",
              "      <th>eta</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>132784</th>\n",
              "      <td>-0.688496</td>\n",
              "      <td>-0.607629</td>\n",
              "      <td>0.868107</td>\n",
              "      <td>0.759040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99666</th>\n",
              "      <td>-0.587358</td>\n",
              "      <td>-0.612672</td>\n",
              "      <td>-1.487534</td>\n",
              "      <td>0.117474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26629</th>\n",
              "      <td>1.051897</td>\n",
              "      <td>1.503479</td>\n",
              "      <td>-1.081401</td>\n",
              "      <td>0.773105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80473</th>\n",
              "      <td>0.788036</td>\n",
              "      <td>1.697702</td>\n",
              "      <td>-0.911068</td>\n",
              "      <td>1.813972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48229</th>\n",
              "      <td>-0.578692</td>\n",
              "      <td>-0.628716</td>\n",
              "      <td>1.619709</td>\n",
              "      <td>-0.830115</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               m        pt       phi       eta\n",
              "132784 -0.688496 -0.607629  0.868107  0.759040\n",
              "99666  -0.587358 -0.612672 -1.487534  0.117474\n",
              "26629   1.051897  1.503479 -1.081401  0.773105\n",
              "80473   0.788036  1.697702 -0.911068  1.813972\n",
              "48229  -0.578692 -0.628716  1.619709 -0.830115"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0zC9ABOJ4Tw",
        "colab_type": "text"
      },
      "source": [
        "#Databunch prepration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuYK2mWxMCA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = TensorDataset(torch.tensor(train_df.values).float(), torch.tensor(train_y.values).float())\n",
        "valid_ds = TensorDataset(torch.tensor(test_df.values).float(), torch.tensor(test_y.values).float())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-odCGz4iUBaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs = 1024 # batch size\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXXcED-jUCig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "databunch = basic_data.DataBunch(train_dl, valid_dl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltylHgNeKBN4",
        "colab_type": "text"
      },
      "source": [
        "#Model definaiton\n",
        "\n",
        "Here I have used a lot of `nn.Linear`, `nn.BatchNorm1d` and `nn.Tanh` layers. Basically `nn.Tanh` is there to add a nonlinearity.\n",
        "\n",
        "### Why we need to add `non-linearity`?\n",
        "Because without non-linearity the model would become a bunch of linear functions(layers here) and combination of more than 1 linar function is again a linear function. There are other non-linear functions like `nn.ReLU` as well but `nn.Tenh` turned out to work better in this case when I tested.\n",
        "\n",
        "### Why we need to use `BatchNorm`?\n",
        "As we normalize our input data for make the learning process faster, Batchnorm does the same thing for the hidden layers. It normalizes the activations to make the learning faster and easier.\n",
        "\n",
        "More details about `BatchNorm` and `non-linearities` can be found on Jeremy Howard's (__Fastai's__) __Practical Deep Learning for Coders 2019 course__. _Find the respective lecture_ [here](https://course.fast.ai/videos/?lesson=6)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8GTAZAzUF4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lin_batchnorms_tanh(nn.Module):\n",
        "    def __init__(self, in_features=7, out_features=3):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(7, 200),\n",
        "            nn.BatchNorm1d(200),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(200, 500),\n",
        "            nn.BatchNorm1d(500),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(500, 500),\n",
        "            nn.BatchNorm1d(500),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(500, 200),\n",
        "            nn.BatchNorm1d(200),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(200, 100),\n",
        "            nn.BatchNorm1d(100),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(100, 50),\n",
        "            nn.BatchNorm1d(50),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(50, 3)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(3, 50),\n",
        "            nn.BatchNorm1d(50),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(50, 100),\n",
        "            nn.BatchNorm1d(100),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(100, 200),\n",
        "            nn.BatchNorm1d(200),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(200, 500),\n",
        "            nn.BatchNorm1d(500),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(500, 500),\n",
        "            nn.BatchNorm1d(500),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(500, 200),\n",
        "            nn.BatchNorm1d(200),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(200, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        return self.decoder(encoded)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2EUbvcmUttd",
        "colab_type": "text"
      },
      "source": [
        "# Learner defination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxscmSG_WO7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Learner = basic_train.Learner\n",
        "learn = Learner(db, Lin_batchnorms_tanh(), loss_func=nn.MSELoss(), bn_wd=False, true_wd=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-E_W7OPMRGL",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTs6yTywV3Qq",
        "colab_type": "text"
      },
      "source": [
        "## Find a good learning rate and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lMGcmkuW-A2",
        "colab_type": "code",
        "outputId": "93760500-ff70-44f7-8a83-7dbc59fb00ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "from fastai import train\n",
        "train.lr_find(learn)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/1 00:00<00:00]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='81' class='' max='110', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      73.64% [81/110 00:03<00:01 0.6717]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e8H0gMxXEaO",
        "colab_type": "code",
        "outputId": "bd8aec64-2a36-4ed3-de7f-a3a724fdc8eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "learn.recorder.plot()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICIS5hQEIiJh7v8BVXlxTCWXLMsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qJ6MokAiPjb",
        "colab_type": "text"
      },
      "source": [
        "#### One cycle training approch\n",
        "\n",
        "Here I am using the one cycle training approch.\n",
        "\n",
        "\n",
        "One cycle training policy was a research paper came in _march, 2018_. __It basically makes the learning rate higher and then again lower as the training process goes on__.\n",
        "\n",
        "_The research paper can be found_ [here](https://arxiv.org/abs/1803.09820)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AtxVf6cXkOR",
        "colab_type": "code",
        "outputId": "46cf7da6-9049-4907-b039-c7bec19a9c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learn.fit_one_cycle(150, 1e-3, wd=1e-6)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.146703</td>\n",
              "      <td>0.115046</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.075202</td>\n",
              "      <td>0.055212</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.047036</td>\n",
              "      <td>0.039672</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.036274</td>\n",
              "      <td>0.033607</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.032941</td>\n",
              "      <td>0.030224</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.030750</td>\n",
              "      <td>0.029033</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.029138</td>\n",
              "      <td>0.027516</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.028476</td>\n",
              "      <td>0.030720</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.026499</td>\n",
              "      <td>0.029208</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.026917</td>\n",
              "      <td>0.025516</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.024652</td>\n",
              "      <td>0.026274</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.024461</td>\n",
              "      <td>0.025269</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.024493</td>\n",
              "      <td>0.027186</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.024186</td>\n",
              "      <td>0.025706</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.023942</td>\n",
              "      <td>0.031259</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.024056</td>\n",
              "      <td>0.027944</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.024325</td>\n",
              "      <td>0.029429</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.024617</td>\n",
              "      <td>0.026730</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.023468</td>\n",
              "      <td>0.026782</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.022495</td>\n",
              "      <td>0.027908</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.023324</td>\n",
              "      <td>0.023644</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.023911</td>\n",
              "      <td>0.028489</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.024068</td>\n",
              "      <td>0.043088</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.022392</td>\n",
              "      <td>0.023713</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.023302</td>\n",
              "      <td>0.039504</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.021905</td>\n",
              "      <td>0.030439</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.021998</td>\n",
              "      <td>0.033833</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.023556</td>\n",
              "      <td>0.027327</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.022302</td>\n",
              "      <td>0.030639</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.022091</td>\n",
              "      <td>0.028622</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.023317</td>\n",
              "      <td>0.085610</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.024841</td>\n",
              "      <td>0.024172</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.021225</td>\n",
              "      <td>0.034237</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.022680</td>\n",
              "      <td>0.033106</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.022365</td>\n",
              "      <td>0.032263</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.022111</td>\n",
              "      <td>0.023926</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.020366</td>\n",
              "      <td>0.024864</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.022344</td>\n",
              "      <td>0.027399</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.022077</td>\n",
              "      <td>0.036715</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.021014</td>\n",
              "      <td>0.029409</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.019100</td>\n",
              "      <td>0.037381</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.020905</td>\n",
              "      <td>0.039653</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.022175</td>\n",
              "      <td>0.038954</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.019056</td>\n",
              "      <td>0.028158</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.018989</td>\n",
              "      <td>0.043640</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.019005</td>\n",
              "      <td>0.028340</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.018036</td>\n",
              "      <td>0.021047</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.017317</td>\n",
              "      <td>0.028180</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.018054</td>\n",
              "      <td>0.028171</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.018763</td>\n",
              "      <td>0.027037</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.017221</td>\n",
              "      <td>0.020865</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.018747</td>\n",
              "      <td>0.042925</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.016472</td>\n",
              "      <td>0.022909</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.017002</td>\n",
              "      <td>0.025507</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.017101</td>\n",
              "      <td>0.038555</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.018018</td>\n",
              "      <td>0.025076</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.018104</td>\n",
              "      <td>0.027213</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.016301</td>\n",
              "      <td>0.018807</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.015220</td>\n",
              "      <td>0.018343</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.015328</td>\n",
              "      <td>0.015436</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.015321</td>\n",
              "      <td>0.018456</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.015926</td>\n",
              "      <td>0.018538</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.015140</td>\n",
              "      <td>0.022615</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.016096</td>\n",
              "      <td>0.030534</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.017547</td>\n",
              "      <td>0.023343</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.015898</td>\n",
              "      <td>0.016436</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.014520</td>\n",
              "      <td>0.014018</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.014396</td>\n",
              "      <td>0.015462</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.014257</td>\n",
              "      <td>0.018306</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.014750</td>\n",
              "      <td>0.029901</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.016708</td>\n",
              "      <td>0.021757</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.013965</td>\n",
              "      <td>0.015517</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.013295</td>\n",
              "      <td>0.013726</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.013939</td>\n",
              "      <td>0.016580</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.013375</td>\n",
              "      <td>0.012967</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.012779</td>\n",
              "      <td>0.015702</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.012883</td>\n",
              "      <td>0.016446</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.013576</td>\n",
              "      <td>0.025561</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.012581</td>\n",
              "      <td>0.015078</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.012600</td>\n",
              "      <td>0.011812</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.012814</td>\n",
              "      <td>0.015235</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.013390</td>\n",
              "      <td>0.011309</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.012487</td>\n",
              "      <td>0.013699</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.012688</td>\n",
              "      <td>0.014881</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.011953</td>\n",
              "      <td>0.012347</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.011650</td>\n",
              "      <td>0.011919</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.011906</td>\n",
              "      <td>0.012553</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.011491</td>\n",
              "      <td>0.011737</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.011730</td>\n",
              "      <td>0.013562</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.010943</td>\n",
              "      <td>0.010096</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.011260</td>\n",
              "      <td>0.010995</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.010916</td>\n",
              "      <td>0.012201</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.010768</td>\n",
              "      <td>0.010317</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.011165</td>\n",
              "      <td>0.011042</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.011454</td>\n",
              "      <td>0.013147</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.010355</td>\n",
              "      <td>0.010159</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.010759</td>\n",
              "      <td>0.009284</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.010770</td>\n",
              "      <td>0.008722</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.010418</td>\n",
              "      <td>0.010415</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.010021</td>\n",
              "      <td>0.012117</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.009990</td>\n",
              "      <td>0.009680</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.009864</td>\n",
              "      <td>0.009286</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.009934</td>\n",
              "      <td>0.008770</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.009772</td>\n",
              "      <td>0.010863</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.009666</td>\n",
              "      <td>0.010599</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.009532</td>\n",
              "      <td>0.008005</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.009318</td>\n",
              "      <td>0.009079</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.010028</td>\n",
              "      <td>0.010129</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.009469</td>\n",
              "      <td>0.009442</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.009856</td>\n",
              "      <td>0.009333</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.010184</td>\n",
              "      <td>0.009341</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.009743</td>\n",
              "      <td>0.010491</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.008753</td>\n",
              "      <td>0.008335</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.008746</td>\n",
              "      <td>0.007492</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.009058</td>\n",
              "      <td>0.007593</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.008492</td>\n",
              "      <td>0.008627</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.008581</td>\n",
              "      <td>0.007959</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.008295</td>\n",
              "      <td>0.008317</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.008768</td>\n",
              "      <td>0.008412</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.008647</td>\n",
              "      <td>0.007796</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.008787</td>\n",
              "      <td>0.007705</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.008234</td>\n",
              "      <td>0.007330</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.008078</td>\n",
              "      <td>0.006925</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.007787</td>\n",
              "      <td>0.007299</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.007674</td>\n",
              "      <td>0.007174</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.007735</td>\n",
              "      <td>0.006939</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.007435</td>\n",
              "      <td>0.006861</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.007795</td>\n",
              "      <td>0.006929</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.007810</td>\n",
              "      <td>0.006993</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.007713</td>\n",
              "      <td>0.007008</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.007884</td>\n",
              "      <td>0.007069</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.007896</td>\n",
              "      <td>0.007049</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.007462</td>\n",
              "      <td>0.006765</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.007290</td>\n",
              "      <td>0.006396</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.007354</td>\n",
              "      <td>0.006523</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.007235</td>\n",
              "      <td>0.006393</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.007218</td>\n",
              "      <td>0.006397</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.007171</td>\n",
              "      <td>0.006287</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.007143</td>\n",
              "      <td>0.006271</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.007146</td>\n",
              "      <td>0.006410</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.007013</td>\n",
              "      <td>0.006248</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.006834</td>\n",
              "      <td>0.006111</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.006931</td>\n",
              "      <td>0.006247</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.006902</td>\n",
              "      <td>0.006174</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.006970</td>\n",
              "      <td>0.006173</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.006728</td>\n",
              "      <td>0.006211</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.006909</td>\n",
              "      <td>0.006250</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.006976</td>\n",
              "      <td>0.006168</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.006978</td>\n",
              "      <td>0.006198</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.007009</td>\n",
              "      <td>0.006244</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErN73Mauc7JA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save(\"AE-7\") # save the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGm0ffF-c_cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mkdir '/content/drive/My Drive/AE-models/'\n",
        "!cp '/content/models/AE-7.pth' '/content/drive/My Drive/AE-models/' # take a copy on the drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r67aMIf1jGb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e0827ca2-142f-4e4c-974c-14813028132d"
      },
      "source": [
        "learn.load(\"AE-7\")"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Learner(data=DataBunch;\n",
              "\n",
              "Train: <torch.utils.data.dataset.TensorDataset object at 0x7f3050760ef0>;\n",
              "\n",
              "Valid: <torch.utils.data.dataset.TensorDataset object at 0x7f30507910b8>;\n",
              "\n",
              "Test: None, model=Lin_batchnorms_tanh(\n",
              "  (encoder): Sequential(\n",
              "    (0): Linear(in_features=7, out_features=200, bias=True)\n",
              "    (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): Tanh()\n",
              "    (3): Linear(in_features=200, out_features=500, bias=True)\n",
              "    (4): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): Tanh()\n",
              "    (6): Linear(in_features=500, out_features=500, bias=True)\n",
              "    (7): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): Tanh()\n",
              "    (9): Linear(in_features=500, out_features=200, bias=True)\n",
              "    (10): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): Tanh()\n",
              "    (12): Linear(in_features=200, out_features=100, bias=True)\n",
              "    (13): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (14): Tanh()\n",
              "    (15): Linear(in_features=100, out_features=50, bias=True)\n",
              "    (16): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (17): Tanh()\n",
              "    (18): Linear(in_features=50, out_features=3, bias=True)\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
              "    (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): Tanh()\n",
              "    (3): Linear(in_features=50, out_features=100, bias=True)\n",
              "    (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): Tanh()\n",
              "    (6): Linear(in_features=100, out_features=200, bias=True)\n",
              "    (7): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): Tanh()\n",
              "    (9): Linear(in_features=200, out_features=500, bias=True)\n",
              "    (10): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): Tanh()\n",
              "    (12): Linear(in_features=500, out_features=500, bias=True)\n",
              "    (13): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (14): Tanh()\n",
              "    (15): Linear(in_features=500, out_features=200, bias=True)\n",
              "    (16): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (17): Tanh()\n",
              "    (18): Linear(in_features=200, out_features=4, bias=True)\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=MSELoss(), metrics=[], true_wd=True, bn_wd=False, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
              "  (0): Linear(in_features=7, out_features=200, bias=True)\n",
              "  (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): Tanh()\n",
              "  (3): Linear(in_features=200, out_features=500, bias=True)\n",
              "  (4): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (5): Tanh()\n",
              "  (6): Linear(in_features=500, out_features=500, bias=True)\n",
              "  (7): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (8): Tanh()\n",
              "  (9): Linear(in_features=500, out_features=200, bias=True)\n",
              "  (10): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (11): Tanh()\n",
              "  (12): Linear(in_features=200, out_features=100, bias=True)\n",
              "  (13): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (14): Tanh()\n",
              "  (15): Linear(in_features=100, out_features=50, bias=True)\n",
              "  (16): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (17): Tanh()\n",
              "  (18): Linear(in_features=50, out_features=3, bias=True)\n",
              "  (19): Linear(in_features=3, out_features=50, bias=True)\n",
              "  (20): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (21): Tanh()\n",
              "  (22): Linear(in_features=50, out_features=100, bias=True)\n",
              "  (23): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (24): Tanh()\n",
              "  (25): Linear(in_features=100, out_features=200, bias=True)\n",
              "  (26): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (27): Tanh()\n",
              "  (28): Linear(in_features=200, out_features=500, bias=True)\n",
              "  (29): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (30): Tanh()\n",
              "  (31): Linear(in_features=500, out_features=500, bias=True)\n",
              "  (32): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (33): Tanh()\n",
              "  (34): Linear(in_features=500, out_features=200, bias=True)\n",
              "  (35): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (36): Tanh()\n",
              "  (37): Linear(in_features=200, out_features=4, bias=True)\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48Of2Ju8X8Ge",
        "colab_type": "code",
        "outputId": "d3448c8b-1ad1-44fe-9630-2ade004a2965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      0.00% [0/1 00:00<00:00]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='62' class='' max='110', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      56.36% [62/110 00:02<00:02 0.0232]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAlQ-2nMYW1q",
        "colab_type": "code",
        "outputId": "d61c95ed-a9cf-42f7-a26e-b8a0308fe656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "learn.recorder.plot()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEGCAYAAABCa2PoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUKCIiEhcKFBERCQu/j/Ynp+T1OEi9wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fPa2a9ugUd4",
        "colab_type": "text"
      },
      "source": [
        "The loss is not decreasing on further training. So not saving the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id1H6BOpnzuF",
        "colab_type": "code",
        "outputId": "6015334e-c052-40a4-a755-119c9f51b345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learn.fit_one_cycle(100, 1e-7)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='24' class='' max='100', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      24.00% [24/100 01:55<06:06]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.006981</td>\n",
              "      <td>0.006295</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.007248</td>\n",
              "      <td>0.006222</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.006917</td>\n",
              "      <td>0.006239</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.006868</td>\n",
              "      <td>0.006116</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.007004</td>\n",
              "      <td>0.006177</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.007004</td>\n",
              "      <td>0.006293</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.006864</td>\n",
              "      <td>0.006103</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.006903</td>\n",
              "      <td>0.006196</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.006854</td>\n",
              "      <td>0.006190</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.006783</td>\n",
              "      <td>0.006139</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.006943</td>\n",
              "      <td>0.006236</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.006923</td>\n",
              "      <td>0.006194</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.006832</td>\n",
              "      <td>0.006257</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.006944</td>\n",
              "      <td>0.006264</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.006990</td>\n",
              "      <td>0.006275</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.006926</td>\n",
              "      <td>0.006266</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.007057</td>\n",
              "      <td>0.006198</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.006841</td>\n",
              "      <td>0.006149</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.006938</td>\n",
              "      <td>0.006215</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.006902</td>\n",
              "      <td>0.006267</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.006975</td>\n",
              "      <td>0.006240</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.006818</td>\n",
              "      <td>0.006178</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.006929</td>\n",
              "      <td>0.006188</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.006818</td>\n",
              "      <td>0.006082</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='103' class='' max='110', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      93.64% [103/110 00:04<00:00 0.0069]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-c8d9b1b4873f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     22\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m def fit_fc(learn:Learner, tot_epochs:int=1, lr:float=defaults.lr,  moms:Tuple[float,float]=(0.95,0.85), start_pct:float=0.72,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_bwd\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/callback.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpg2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_DLfNnD3QFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.save(\"AE-8\") # save the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB94NeHt3Um5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/models/AE-8.pth' '/content/drive/My Drive/AE-models/' # take a copy on the drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAj6tyAdWKWU",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZpyMwqW3og4",
        "colab_type": "text"
      },
      "source": [
        "### Predicitons on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXfocc_MZrTh",
        "colab_type": "code",
        "outputId": "2526b2a5-54ee-4cc4-85e8-c06b50b049f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "learn.get_preds(ds_type=DatasetType.Valid)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[-0.5259, -0.5867,  1.0892, -0.0638],\n",
              "         [-0.4781, -0.5719,  0.5686,  0.4757],\n",
              "         [-0.4962, -0.5798, -0.6030,  0.5521],\n",
              "         ...,\n",
              "         [-0.5387, -0.5455, -1.2283, -1.6014],\n",
              "         [-0.1429, -0.3979, -1.4853, -0.5979],\n",
              "         [ 0.0705, -0.2117, -1.0918,  0.3654]]),\n",
              " tensor([[-0.5333, -0.5819,  1.0872, -0.0711],\n",
              "         [-0.4724, -0.6093,  0.5733,  0.4770],\n",
              "         [-0.4988, -0.5703, -0.6227,  0.5422],\n",
              "         ...,\n",
              "         [-0.5517, -0.5617, -1.2107, -1.6051],\n",
              "         [-0.1767, -0.3924, -1.4821, -0.6032],\n",
              "         [ 0.0446, -0.1671, -1.1240,  0.3776]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eF1YC1T37n8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "6b3acabf-ed63-4787-b521-000a1e8f01d5"
      },
      "source": [
        "plt.plot(learn.recorder.val_losses, marker='>', label='Validation')\n"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-126-47f7b28ae999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Learner' object has no attribute 'val_losses'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZdshcWM4pJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}